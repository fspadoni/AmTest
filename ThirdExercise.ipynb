{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third exercise: plot the monthly number of searches for flights arriving at Málaga, Madrid or Barcelona\n",
    "\n",
    "For the arriving airport, you can use the Destination column in the searches file. Plot a curve for Málaga, another one for Madrid, and another one for Barcelona, in the same figure. Bonus point: Solving this problem using pandas (instead of any other approach) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20390198"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val searches = sc.textFile(\"searches.csv.bz2\")\n",
    "val cleandata = searches.map(line => line.split(\"\\\\^\").map(_.trim))\n",
    "val firstRow = cleandata.first\n",
    "val data = cleandata.filter( line => !line.contains(firstRow(0)) )\n",
    "data.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2013-01-01, 20:25:57, MPT, 624d8c3ac0b3a7ca03e3c167e0f48327, DE, TXL, AUH, 1, 2, TXL, AUH, 2013-01-26, D2, \"\", AUH, TXL, 2013-02-02, D2, \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", 1ASIWS, 0, 0, 0, d41d8cd98f00b204e9800998ecf8427e, FRA)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numberFields = firstRow.length   // number of columns excpected for each line\n",
    "val filteredData = data.filter( line => line.length >= numberFields) \n",
    "filteredData.first\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(MAD,2013-01-01)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val destination = firstRow.indexOf(\"Destination\") // index of arrival airport column in csv data file\n",
    "val date = firstRow.indexOf(\"Date\")           // index of number of passangers column in csv data file\n",
    "\n",
    "val keyValRDD = filteredData.map{ x => (x(destination), x(date) ) }\n",
    "keyValRDD.count\n",
    "\n",
    "val madridSearches = keyValRDD.filter( kv => kv._1 == \"MAD\" )\n",
    "madridSearches.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "def getSearchesPerMonth( searches: RDD[(String, java.util.Date)], month: Int) : Long =\n",
    "{\n",
    "    return searches.filter( kv=> kv._2.getMonth() == month ).count\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getSearchesForAirport( searches: RDD[(String, String)], airport: String) : RDD[(String, java.util.Date)] =\n",
    "{\n",
    "    val searchesAirport = searches.filter( kv => kv._1 == airport )\n",
    "    val format = new java.text.SimpleDateFormat(\"yyyy-MM-dd\")\n",
    "    val returnValue = searchesAirport.mapValues( v => format.parse( v ));\n",
    "    return returnValue\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Syntax Error.\n",
       "Message: \n",
       "StackTrace: "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val madridSearchesDate = getSearchesForAirport(keyValRDD, \"MAD\")\n",
    "madridSearchesDate.first\n",
    "//val madridSearchesMonth = madridSearchesDate.filter( kv=> kv._2.getMonth() == 0 )\n",
    "//madridSearchesMonth.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month\tMalaga\tMadrid\tBarcellona\n",
      "Jan\t9633\t24258\t29469\n",
      "Feb\t8379\t22800\t28329\n"
     ]
    }
   ],
   "source": [
    "val monthNumberToName = Array( \"Jan\",\"Feb\", \"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\")\n",
    "  \n",
    "//val madridSearches = keyValRDD.filter( kv => kv._1 == \"MAD\" )\n",
    "//val madridSearchesDate = madridSearches.mapValues( v => format.parse( v ))\n",
    "\n",
    "val madridSearches = getSearchesForAirport(keyValRDD, \"MAD\")\n",
    "val barcellonaSearches = getSearchesForAirport(keyValRDD, \"BCN\" )\n",
    "val malagaSearches = getSearchesForAirport( keyValRDD, \"AGP\" )\n",
    "\n",
    "println( \"Month\\tMalaga\\tMadrid\\tBarcellona\")\n",
    "\n",
    "for( month <- 0 to monthNumberToName.length-11)\n",
    "{\n",
    "    println( monthNumberToName(month) \n",
    "    + \"\\t\" + getSearchesPerMonth(malagaSearches, month) \n",
    "    + \"\\t\" + getSearchesPerMonth(madridSearches, month)\n",
    "    + \"\\t\" + getSearchesPerMonth(barcellonaSearches, month) \n",
    "    )\n",
    "    \n",
    "}\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 129, localhost): java.lang.IllegalArgumentException: Cannot format given Object as a Date\n",
       "\tat java.text.DateFormat.format(DateFormat.java:310)\n",
       "\tat java.text.Format.format(Format.java:157)\n",
       "\tat $line157.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:43)\n",
       "\tat $line157.$read$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:43)\n",
       "\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n",
       "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:312)\n",
       "\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n",
       "\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)\n",
       "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)\n",
       "\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)\n",
       "\tat scala.collection.AbstractIterator.to(Iterator.scala:1157)\n",
       "\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)\n",
       "\tat scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)\n",
       "\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)\n",
       "\tat scala.collection.AbstractIterator.toArray(Iterator.scala:1157)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1328)\n",
       "\tat org.apache.spark.rdd.RDD$$anonfun$take$1$$anonfun$28.apply(RDD.scala:1328)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)\n",
       "\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "\tat java.lang.Thread.run(Thread.java:745)\n",
       "\n",
       "Driver stacktrace:\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n",
       "org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1328)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.take(RDD.scala:1302)\n",
       "org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1342)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.first(RDD.scala:1341)\n",
       "$line158.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:46)\n",
       "$line158.$read$$iwC$$iwC$$iwC.<init>(<console>:51)\n",
       "$line158.$read$$iwC$$iwC.<init>(<console>:53)\n",
       "$line158.$read$$iwC.<init>(<console>:55)\n",
       "$line158.$read.<init>(<console>:57)\n",
       "$line158.$read$.<init>(<console>:61)\n",
       "$line158.$read$.<clinit>(<console>)\n",
       "$line158.$eval$.<init>(<console>:7)\n",
       "$line158.$eval$.<clinit>(<console>)\n",
       "$line158.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val monthFormat = new java.text.SimpleDateFormat(\"MM-dd-yyyy\");\n",
    "val madridSearchesDateMonth = madridSearches.map(kv => monthFormat.format( kv._2 ))\n",
    "madridSearchesDateMonth.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: Compile Error\n",
       "Message: <console>:31: error: type mismatch;\n",
       " found   : () => String\n",
       " required: Array[String] => ?\n",
       "       val numbSearches = data.map( Utils.getAirport(\"MAD\").toString )\n",
       "                                                            ^\n",
       "StackTrace: "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object Utils {\n",
    "    \n",
    "    /**\n",
    "    def getAirportRDD( data: RDD[Array[String]], airport: String) : array[long] = \n",
    "    {\n",
    "    \n",
    "        val numberSearches = Array(1, 2, 3)\n",
    "        return numberSearches\n",
    "    }\n",
    "    */\n",
    "    \n",
    "    def getAirport( airport: String) : Array[Int] = \n",
    "    {\n",
    "    \n",
    "        val numberSearches = Array(1, 2, 3)\n",
    "        return numberSearches\n",
    "    }\n",
    "}\n",
    "\n",
    "val numbSearches = data.map( Utils.getAirport(\"MAD\").toString )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "//val madridSearches = data.map( line => line.contains(\"MAD\") )\n",
    "//numbSearches.count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.6.1 (Scala)",
   "language": "",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
