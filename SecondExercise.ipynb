{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second exercise: top 10 arrival airports in the world in 2013 (using the bookings file)\n",
    "\n",
    "Arrival airport is the column arr_port. It is the IATA code for the airport\n",
    "\n",
    "To get the total number of passengers for an airport, you can sum the column pax, grouping by arr_port. Note that there is negative pax. That corresponds to cancelations. So to get the total number of passengers that have actually booked, you should sum including the negatives (that will remove the canceled bookings).\n",
    "\n",
    "Print the top 10 arrival airports in the standard output, including the number of passengers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Load and clean data: remove the first line and the delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val bookings = sc.textFile(\"bookings.csv.bz2\")\n",
    "val cleandata = bookings.map(line => line.split(\"\\\\^\").map(_.trim))\n",
    "val firstRow = cleandata.first\n",
    "val data = cleandata.filter( line => line(0) != firstRow(0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if the first line is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2013-03-05 00:00:00, 1A, DE, a68dd7ae953c8acfb187a1af2dcbe123, 1a11ae49fcbf545fd2afc1a24d88d2b7, ea65900e72d71f4626378e2ebd298267, 2013-02-22 00:00:00, 1708, 0, ZRH, ZRH, CH, LHR, LON, GB, ZRH, ZRH, CH, LHR, LON, GB, ZRH, ZRH, CH, LHRZRH, LONZRH, CHGB, 1, LHRZRH, VI, T, Y, 2013-03-07 08:50:00, 2013-03-07 11:33:37, -1, 2013, 3, NULL)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check if the number of lines are correct ( bookings.count - 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000010"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map(off_port -> ZRH, duration -> 1708, brd_ctry -> GB, lst_city -> ZRH, pos_ctry -> DE, source -> 1A, mkt_port -> LHRZRH, carrier -> VI, arr_ctry -> GB, pos_oid -> 1a11ae49fcbf545fd2afc1a24d88d2b7, lst_ctry -> CH, oid -> NULL, arr_city -> LON, cre_date -> 2013-02-22 00:00:00, year -> 2013, mkt_ctry -> CHGB, cab_class -> Y, rloc -> ea65900e72d71f4626378e2ebd298267, dep_city -> ZRH, pax -> -1, intl -> 1, pos_iata -> a68dd7ae953c8acfb187a1af2dcbe123, off_time -> 2013-03-07 11:33:37, act_date -> 2013-03-05 00:00:00, arr_port -> LHR, lst_port -> ZRH, dep_port -> ZRH, brd_city -> LON, dep_ctry -> CH, route -> LHRZRH, bkg_class -> T, off_ctry -> CH, mkt_city -> LONZRH, distance -> 0, brd_port -> LHR, month -> 3, brd_time -> 2013-03-07 08:50..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val maps = data.map( line => firstRow.zip(line).toMap )\n",
    "maps.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val cleanMaps = maps.map{ x => ( \"pax\", \"arr_port\") }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(pax,arr_port)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanMaps.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0.0 in stage 20.0 (TID 66) had a not serializable result: scala.collection.immutable.MapLike$$anon$1\n",
       "Serialization stack:\n",
       "\t- object not serializable (class: scala.collection.immutable.MapLike$$anon$1, value: Map(pax -> -1, arr_port -> LHR))\n",
       "\t- element of array (index: 0)\n",
       "\t- array (class [Lscala.collection.immutable.Map;, size 1)\n",
       "StackTrace: org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n",
       "scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n",
       "org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n",
       "scala.Option.foreach(Option.scala:236)\n",
       "org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n",
       "org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n",
       "org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n",
       "org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n",
       "org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1328)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.take(RDD.scala:1302)\n",
       "org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1342)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n",
       "org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n",
       "org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n",
       "org.apache.spark.rdd.RDD.first(RDD.scala:1341)\n",
       "$line76.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:35)\n",
       "$line76.$read$$iwC$$iwC$$iwC.<init>(<console>:40)\n",
       "$line76.$read$$iwC$$iwC.<init>(<console>:42)\n",
       "$line76.$read$$iwC.<init>(<console>:44)\n",
       "$line76.$read.<init>(<console>:46)\n",
       "$line76.$read$.<init>(<console>:50)\n",
       "$line76.$read$.<clinit>(<console>)\n",
       "$line76.$eval$.<init>(<console>:7)\n",
       "$line76.$eval$.<clinit>(<console>)\n",
       "$line76.$eval.$print(<console>)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "java.lang.reflect.Method.invoke(Method.java:498)\n",
       "org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n",
       "org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n",
       "org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n",
       "org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:361)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1$$anonfun$apply$3.apply(ScalaInterpreter.scala:356)\n",
       "org.apache.toree.global.StreamState$.withStreams(StreamState.scala:81)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.kernel.interpreter.scala.ScalaInterpreter$$anonfun$interpretAddTask$1.apply(ScalaInterpreter.scala:355)\n",
       "org.apache.toree.utils.TaskManager$$anonfun$add$2$$anon$1.run(TaskManager.scala:140)\n",
       "java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n",
       "java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n",
       "java.lang.Thread.run(Thread.java:745)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filterKey(key: Map[String,String])  : Boolean = {\n",
    "    if (key contains \"pax\") return true\n",
    "    else if (key contains \"arr_port\") return true\n",
    "    else return false\n",
    "  }\n",
    "  \n",
    "def filterKeyV(key: Map[String,String])  = {\n",
    "    val interestingKeys = List(\"pax\", \"arr_port\")\n",
    "    key.filterKeys(interestingKeys.contains)\n",
    "  }\n",
    "  \n",
    "  val rmap = maps.map(x => filterKeyV(x))\n",
    "  rmap.first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.6.1 (Scala)",
   "language": "",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
