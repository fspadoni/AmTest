{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second exercise: top 10 arrival airports in the world in 2013 (using the bookings file)\n",
    "\n",
    "Arrival airport is the column arr_port. It is the IATA code for the airport\n",
    "\n",
    "To get the total number of passengers for an airport, you can sum the column pax, grouping by arr_port. Note that there is negative pax. That corresponds to cancelations. So to get the total number of passengers that have actually booked, you should sum including the negatives (that will remove the canceled bookings).\n",
    "\n",
    "Print the top 10 arrival airports in the standard output, including the number of passengers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Load and clean data: remove the first line and the delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val bookings = sc.textFile(\"bookings.csv.bz2\")\n",
    "val cleandata = bookings.map(line => line.split(\"\\\\^\").map(_.trim))\n",
    "val firstRow = cleandata.first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the first line of the file (header) in this inline function can fail.\n",
    "Spark can complain the \"NotSerializableException occurs in Scala\" it means that a reference to firstRow:Array[String] is not serializable. \n",
    "It's still not clear to me but I think is a Spark environment configuration problem\n",
    "\n",
    "In the second line there is the alternative version always working just passing the literal of the first string in firstRow array. This version can't be used in general case to remove the first line of a file loaded in a RDD.\n",
    "\n",
    "Anyway I am sure or at least I excpect there is a more efficient way to remove the first line of a file loaded in a RDD in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val data = cleandata.filter( line => !line.contains(firstRow(0)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val data = cleandata.filter( line => !line.contains(\"act_date\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if the first line is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2013-03-05 00:00:00, 1A, DE, a68dd7ae953c8acfb187a1af2dcbe123, 1a11ae49fcbf545fd2afc1a24d88d2b7, ea65900e72d71f4626378e2ebd298267, 2013-02-22 00:00:00, 1708, 0, ZRH, ZRH, CH, LHR, LON, GB, ZRH, ZRH, CH, LHR, LON, GB, ZRH, ZRH, CH, LHRZRH, LONZRH, CHGB, 1, LHRZRH, VI, T, Y, 2013-03-07 08:50:00, 2013-03-07 11:33:37, -1, 2013, 3, NULL)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if the number of lines are correct ( bookings.count - 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000010"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "processing the lines I got error in reduceByKey() function (java.lang.ArrayIndexOutOfBoundsException)\n",
    "It might be that there are some lines empty or with not enough elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000009"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val numberFields = firstRow.length   // number of columns excpected for each line\n",
    " val filteredData = data.filter( line => line.length >= numberFields) \n",
    " filteredData.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Key Value Pair RDD where key=arr_port  value=pax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.storage.StorageLevel\n",
    " \n",
    "val arr_port = firstRow.indexOf(\"arr_port\") // index of arrival airport column in csv data file\n",
    "val pax = firstRow.indexOf(\"pax\")           // index of number of passangers column in csv data file\n",
    "\n",
    "val keyValRDD = filteredData.map{ x => (x(arr_port), x(pax).toLong ) }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Sum all the values (pax) with the same key (arr_port) and store in keyValReduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2274"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val keyValReduced = keyValRDD.reduceByKey( (x,y) => x + y )\n",
    "//keyValReduced.persist(StorageLevel.MEMORY_ONLY)\n",
    "keyValReduced.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the first 10 arrival just reordering by value the keyValReduced RDD and print the 10 firsts Key Value Pair.\n",
    "it's safe to run foreach because sortedRDD.take(..) move the RDD data on the Driver Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results:\n",
      "\n",
      "(LHR,88809)\n",
      "(MCO,70930)\n",
      "(LAX,70530)\n",
      "(LAS,69630)\n",
      "(JFK,66270)\n",
      "(CDG,64490)\n",
      "(BKK,59460)\n",
      "(MIA,58150)\n",
      "(SFO,58000)\n",
      "(DXB,55590)\n"
     ]
    }
   ],
   "source": [
    "println(\"Results:\\n\")\n",
    "val sortedRDD = keyValReduced.sortBy(  _._2 , false)\n",
    "sortedRDD.take(10).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark 1.6.1 (Scala)",
   "language": "",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "name": "scala",
   "version": "2.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
